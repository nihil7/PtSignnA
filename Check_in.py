import os
import time
import random
import cloudscraper
from datetime import datetime
# from dotenv import load_dotenv

# === åŠ è½½æœ¬åœ° .envï¼ˆGitHub Actions ä¸­è·³è¿‡ï¼‰===
if not os.getenv("GITHUB_ACTIONS", "").lower() == "true":
    load_dotenv()

# === é€šç”¨è¯·æ±‚å¤´ ===
headers = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Mobile Safari/537.36 Edg/135.0.0.0",
    "Referer": "https://pt.btschool.club/index.php",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
}


# === åˆ›å»º Cloudscraper è¯·æ±‚å¯¹è±¡ ===
scraper = cloudscraper.create_scraper()

# === æ¯ä¸ªè´¦å·ä¸€æ¡é…ç½® ===
ACCOUNTS = [
    {
        "site": "PTTIME",
        "account": "A",
        "url": "https://www.pttime.org/attendance.php?type=sign&uid=2785"
    },
    {
        "site": "PTTIME",
        "account": "B",
        "url": "https://www.pttime.org/attendance.php?type=sign&uid=20801"
    },
    {
        "site": "PTBA",
        "account": "X1",
        "url": "https://1ptba.com/attendance.php"
    },
    {
        "site": "PTZONE",
        "account": "X1",
        "url": "https://ptzone.xyz/attendance.php"
    },
    {
        "site": "BTSCHOOL",
        "account": "X1",
        "url": "https://pt.btschool.club/index.php?action=addbonus"
    }
]

# === æ„é€  Cookie ===
def build_cookie(site, account):
    prefix = f"{site}_{account}".upper()
    return {
        'logged_in': os.getenv(f'{prefix}_LOGGED_IN'),
        'cf_clearance': os.getenv(f'{prefix}_CF_CLEARANCE'),
        'c_secure_uid': os.getenv(f'{prefix}_C_SECURE_UID'),
        'c_secure_tracker_ssl': os.getenv(f'{prefix}_C_SECURE_TRACKER_SSL'),
        'c_secure_ssl': os.getenv(f'{prefix}_C_SECURE_SSL'),
        'c_secure_pass': os.getenv(f'{prefix}_C_SECURE_PASS'),
        'c_secure_login': os.getenv(f'{prefix}_C_SECURE_LOGIN'),
        'c_lang_folder': os.getenv(f'{prefix}_C_LANG_FOLDER'),
    }

# === æ ¡éªŒ Cookie ===
def is_cookie_valid(cookie_dict):
    """
    åªéªŒè¯å…³é”®å­—æ®µï¼Œä¸å¼ºåˆ¶è¦æ±‚ cf_clearance
    """
    required = [
        'logged_in',
        'c_secure_uid',
        'c_secure_pass',
        'c_secure_ssl',
        'c_secure_tracker_ssl',
        'c_secure_login'
    ]
    return all(cookie_dict.get(k) for k in required)



# === æ‰“å° Cookieï¼ˆè°ƒè¯•ç”¨ï¼‰===
def print_cookie_info(name, cookie_dict):
    print(f"ğŸª {name} cookies:")
    for k, v in cookie_dict.items():
        print(f"  {k} = {v if v else 'âŒ ç¼ºå¤±'}")

# === æ‰§è¡Œç­¾åˆ° ===
def check_in(url, cookies, site, account):
    print(f"ğŸš€ æ­£åœ¨ç­¾åˆ° [{site} - {account}]: {url}")
    try:
        response = scraper.get(url, cookies=cookies, headers=headers, timeout=10)
        print(f"âœ… çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ å†…å®¹ï¼ˆå‰200å­—ï¼‰: {response.text[:200]}...")
        if response.status_code == 200:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"ğŸ‰ æˆåŠŸç­¾åˆ°ï¼š{site} - {account} @ {now}")
        else:
            print(f"âŒ ç­¾åˆ°å¤±è´¥ï¼š{site} - {account}")
    except Exception as e:
        print(f"ğŸš¨ å¼‚å¸¸ï¼š{site} - {account} - {str(e)}")

# === ä¸»å‡½æ•° ===
def main():
    for conf in ACCOUNTS:
        site = conf["site"]
        account = conf["account"]
        url = conf["url"]

        cookie = build_cookie(site, account)
        print_cookie_info(f"{site}-{account}", cookie)

        if not is_cookie_valid(cookie):
            print(f"âš ï¸ è·³è¿‡ï¼š{site} - {account}ï¼ŒCookie ä¿¡æ¯ä¸å®Œæ•´")
            continue

        check_in(url, cookie, site, account)

        delay = random.uniform(0, 5)
        print(f"â³ ç­‰å¾… {delay:.2f} ç§’...\n")
        time.sleep(delay)

if __name__ == "__main__":
    main()
